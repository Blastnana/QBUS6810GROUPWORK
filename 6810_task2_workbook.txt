### 6810- TASK2- WorkBook ###

Notation: 
- comment, ~.  date & initial. Behind.         ex:  xxxxxx    ~xy. 15 
- readed, $ (smile).  %(xy).  Ahead.           ex: $  or % - porpotion of +ve response in training set: 16.582%
- highlight/request, @.                                            ex: @ --> @@ --> @@@ 
- saved 【csv】 
- ""Always leave a notification summary on README""


Outline:

Part 1 : Biz understanding

Part 2: Data understanding (all data)
	- For Y: 
		- porpotion of +ve response in training set: 16.582% 
		- correlation to Y(response).    【corr to Response.csv  & corr to Response(abs).csv】 
	- For X:
		- Descriptive analysis + skew & kurt.  【descriptive for X_train.csv】

Part 3: Data preparation (variable selection)
	- PCA
		- 需要9个components，更多的几乎没有再有提高
	- forward subset method
	- Logistic Regression. 【‘logit_sum.excl’, 'logit_coef.csv' & 'logit_coef_asb.csv'】 
		- 模型说是的15个，coef是排过序的，别算intercept
		- http://blog.yhat.com/posts/logistic-regression-and-python.html （可参考逻辑&解释）
		- 我用tut10的 uni logit_reg的图画了看了一个，没有一个是好的····· 
	- Decision Tree 【'tree_importency.csv'】
		- 效果特别好，先用了GridSearch搜出来optimal depth是4个
		- 然后找出来了4个非0的，从大到小是LTFREDAY, Days, Fredays, Fre
		- http://songhuiming.github.io/pages/2016/07/12/variable-selection-in-python/ （参照着可解释）
		- 问题是，tree跟logit选出来的variables完全不对着，所以要用两个的话，的想想如何解释
	- Recursive feature eliminaation w/ cv = 5
		-这个就比较尴尬了，跑出来说50个都是相对独立的，都是重要的。。。。
		- http://scikit-learn.org/stable/modules/feature_selection.html#rfe (参考的解释)


Part 4: Modeling (Confidence interval for the expected gross profit from a customer for each model)
	- 6810: Naive Bayes + Logistic Regression +  KNN + Discriment 
	- 6850: Decision Tree + Adaboost + Essemble

Part 5: Evaluation
	- ROC

Part 6: Development
	- Confution matrix
